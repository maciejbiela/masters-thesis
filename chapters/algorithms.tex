\paragraph{}
Here I will describe in details what each algorithm does

\section{OpenCV}
\subsection{Image Thresholding}
\paragraph{Introduction}
Thresholding is a process of creating binary image from a grayscale one.\cite{digital-image-processing} It is the simplest form of segmentation (separating image into regions). It uses the simplest property that pixels in a region can share - the intensity. Hence thresholding is a natural way of separating light and dark regions of the image. In simple words, all pixels with intensity value below some threshold are being assigned zero and all pixels with intensity above this threshold one. Pixels that are equal the threshold value are treated either as zero or one but the behaviour needs to be consistent. 

\begin{equation}
	g(x, y) = \begin{cases}
		1 & \text{if $f(x, y) >= T$}\\
		0 & \text{otherwise}
	\end{cases}
\end{equation}

\paragraph{Problems with thresholding}
The most significant issue with thresholding is the fact that only the intensity is considered and no relationship between pixels is. This can lead to the inclusion of external pixels that are not part of the original region. Similarly, isolated pixels can be lost from a region. The presence of noise in the image can easily worsen the outcome, because pixels intensity may not represent the normal intensity in the region. The usage of thresholding is often based on experimentation and small adjustments. Still, a large portion of a region may be lost or the area may be extended with extraneous background pixels (especially when shadows of objects are presents - which causes them to be included as part of dark object on a light background). One of the flaws of global image thresholding is also the fact that it is not particularly effective when changes in illumination occur in the image. This drawback can partially be mitigated by determining thresholds locally, so instead of a single global threshold value, the threshold itself can actually vary across different parts of the image.

\todo{Add images that showcase different thresholding}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{images/thresholds}
	\caption{Thresholding variants in OpenCV}
\end{figure}

\subsection{Image Filtering}
\paragraph{Introduction}
Filtering is a technique for modifying or enhancing an image. It is used either to emphasize some features of the image or to remove some other. Images can be filtered with various low-pass filters (LPF) or high-pass filters (HPF). HPFs are useful for finding edges in the images whereas LPFs are used to remove noise and for image blurring.

\paragraph{Correlation and convolution}\cite{correlation-convolution}
This are basic operations that can be applied in order to extract information from an image. In a sense they are the simplest operations that can be performed but, nevertheless, extremely powerful and useful. Due to their simplicity they are also well understood, easy implementable and efficiently computable. Both of these operations fullfill two features:
\begin{itemize}
	\item Linearity
	\item Shift-invariance
\end{itemize}
Let us now look at correlation in 2D. Given a square filter with odd number of elements represented by a $(2N + 1)$ x $(2N + 1)$ matrix $F$ and the image matrix $I$, the results of correlation can be computed by aligning the center of the filter with a pixel. Overlapping values are then multiplied together and summed up to make the result corresponding to that given pixel. It can be written as
\begin{equation}
	(F \otimes I)(x, y) = \sum_{i = -N}^{N}\sum_{j = -N}^{N}F(i, j)I(x + i, y + j)
\end{equation}
\todo{Insert example here showing matrices and the operations being applied}
Convolution is very similar to correlation, but the filter is flipped both horizontally and vertically beforehand. 
\begin{equation}
	(F \star I)(x, y) = \sum_{i = -N}^{N}\sum_{j = -N}^{N}F(i, j)I(x - i, y - j)
\end{equation}
The key distinction before the two operations is the associative property of convolution:
\begin{equation}
	\forall F, G - filters \quad \forall I - image: F \star (G \star I) = (F \star G) \star I
\end{equation}
This property is useful when there is need to convolve image with multiple filters. It is computationally faster to precompute the final filter as convolution of original filters and apply just one convolution to the input image. Convolutions are used for image processing operations such as smoothing whereas correlations are useful for matching templates.
One final notice, that for symmetrical filters convolution and correlation are identical.

\paragraph{Image blurring}
To achieve smoothing effect, the image is convolved with a LPF kernel. This particular type of filtering removes high frequency content from the image (for example noise). Unfortunately, edges also can be blurred a bit while applying this operation, although there are also smoothing filters that prevent edges from being blurred. The OpenCV library provides user with various filters amongst which the most popular ones are:
\begin{itemize}
	\item Averaging - it takes the average of all the pixels under kernel area
	\item Gaussian - instead of box filter, a Gaussian kernel is used
	\item Median - in this case the median value of all the pixels under kernel area is used
	\item Bilateral - slower compared to other filters, but it keeps edges sharp
\end{itemize}
\todo{Showcase different filters}

\subsection{Morphological Transformations}
\subsection{Finding Contours}

\subsection{Edge Detection}
\subsubsection{Derivatives and edges}
An edge is a place of a sudden discontinuity in an image, which can arise from surface normal, surface color, depth, illumination, or other discontinuities. This rapid change in the image intensity function can be observed in places where the first derivative of this function has local extrema (see Figure \ref{fig:edge-detection}).\cite{edge-detection}
\begin{figure}[H]
     \centering
     \subfloat[Original Image]{\includegraphics[width=0.5\textwidth]{images/edge_image}}
     \vfill
     \subfloat[Intensity along the specified line]{\includegraphics[width=0.5\textwidth]{images/edge_intensity_function}}
     \vfill
     \subfloat[Derivative of the intensity function]{\includegraphics[width=0.5\textwidth]{images/edge_first_derivative}}
     \caption{Edge detection}
     \label{fig:edge-detection}
\end{figure}

\subsubsection{Image gradient}
The gradient points in the direction of most rapid increase in intensity and can be represented as:
\begin{equation}
	\nabla f = [\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}]
\end{equation}
The direction of the gradient is given by:
\begin{equation}
	\theta = tan^{-1}(\frac{\partial f}{\partial y} / \frac{\partial f}{\partial x})
\end{equation}
And, finally, the \textit{amount of change} is given by the gradient magnitude:
\begin{equation}
	\parallel \nabla f \parallel = \sqrt{(\frac{\partial f}{\partial x})^2 + (\frac{\partial f}{\partial y})^2}
\end{equation}
Remember, that for 2D function partial derivative is defined as:
\begin{equation}
	\frac{\partial f(x,y)}{\partial x} = \lim_{\epsilon \to 0} \frac{f(x + \epsilon, y) - f(x, y)}{\epsilon}
\end{equation}
But for discrete data this definition needs to be approximated using finite differences:
\begin{equation}
	\frac{\partial f(x,y)}{\partial x} \approx \frac{f(x + 1, y) - f(x, y)}{1} \approx f(x + 1, y) - f(x, y)
\end{equation}
In order to obtain an operator (a kernel) that implements the definition of the discrete gradient, we end up with averaging "left" and "right" derivative and obtain the kernel:
\[
\frac{1}{2}
(\begin{bmatrix}
    0 & 0 & 0 \\
    -1 & 1 & 0 \\
    0 & 0 & 0
\end{bmatrix}
+
\begin{bmatrix}
    0 & 0 & 0 \\
    0 & -1 & 1 \\
    0 & 0 & 0
\end{bmatrix})
=
\begin{bmatrix}
    0 & 0 & 0 \\
    \frac{-1}{2} & 0 & \frac{1}{2} \\
    0 & 0 & 0
\end{bmatrix}
\]

{\renewcommand{\arraystretch}{2}
\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		\cellcolor{gray} & \textbf{X} & \textbf{Y} \\
		\hline
		\textbf{Sobel} & $\begin{bmatrix} -1 & 0 & 1 \\ -2 & 0 & 2 \\ -1 & 0 & 1 \end{bmatrix}$ & $\begin{bmatrix} 1 & 2 & 1 \\ 0 & 0 & 0 \\ -1 & -2 & -1\end{bmatrix}$ \\
		\hline
		\textbf{Prewitt} & $\begin{bmatrix} -1 & 0 & 1 \\ -1 & 0 & 1 \\ -1 & 0 & 1 \end{bmatrix}$ & $\begin{bmatrix} 1 & 1 & 1 \\ 0 & 0 & 0 \\ -1 & -1 & -1 \end{bmatrix}$ \\
		\hline
		\textbf{Roberts} & $\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}$ & $\begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}$ \\
		\hline
	\end{tabular}
	\caption{Some of the well-known gradient kernels}
\end{table}
}
In the real world the intensity function is affected by noise and may look like in Figure \ref{fig:noise_intensity_function}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{images/noise_intensity_function}
	\caption{Intensity function affected by noise}
	\label{fig:noise_intensity_function}
\end{figure}
This can cause the result of applying the derivative operator to be unable to tell where the edge might be (see Figure \ref{fig:noise_derivative})
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{images/noise_derivative}
	\caption{Derivative of the intensity function affected by noise}
	\label{fig:noise_derivative}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{images/noise_impact}
	\caption{Noise impact on the gradient image}
	\label{fig:noise_impact}
\end{figure}

\subsubsection{Edge detection in general}
Primary edge detection steps include:
\begin{itemize}
	\item Smoothing derivatives to suppress noise and compute gradient
	\item Threshold to find regions of "significant" gradient
	\item Thinning to get localized edge pixels
	\item Connecting edge pixels
\end{itemize}

\subsubsection{Canny Edge Detection}
In particular, Canny edge operator consists of following steps\cite{canny-edge}:
\begin{enumerate}
	\item Filtering the image with derivative of Gaussian
	\item Finding the magnitude and the orientation of the gradient
	\item Performing non-maximum suppression: thinning multi-pixel wide ridges (as the edges may have been broadened in step 1.) down to single pixel width
	\item Linking and thresholding (hysteresis). To perform this step two thresholds are defined: low and high. Then it continues as follows:
	\begin{enumerate}
		\item The high threshold is applied to detect strong edge pixels
		\item Strong edge pixels are linked to form strong edges
		\item The low threshold is applied to find weak, but plausible, edge pixels
		\item Strong edges are extended to follow weak edge pixels
	\end{enumerate}
\end{enumerate}

\subsection{Hough Lines Detection}
\paragraph{Voting}
is a general technique where we let all the features vote for all models that are compatible with it.
\begin{enumerate}
	\item Cycle through features, each casting votes for model parameters.
	\item Look for model parameters that receive a lot of votes.
\end{enumerate}

\paragraph{Hough Line Transform}
is a voting technique that can be used to answer all of the following questions:
\begin{itemize}
	\item Given points that belong to a line, what is the line?
	\item How many lines are there?
	\item Which points belong to which lines?
\end{itemize}\cite{introduction-to-computer-vision}
In order to use Hough Line Transform, the image should be binary. The most common approach is to convert color image to grayscale, then apply edge detection to obtain the edge pixels. This binary image is then undergoes Hough transformation that outputs a set of lines that were found. The main idea of this algorithm is as follows:
\begin{enumerate}
	\item Each edge pixel votes for compatible lines
	\item Look for lines that got many votes
\end{enumerate}

\paragraph{Line representations}
Let us now look at the possible representations of a line. The simplest and most widely used one is using Cartesian coordinates (see Figure \ref{fig:line_cartesian}):
\begin{equation}
	y = m x + b
\end{equation}
where $m$ represents the slope and $b$ the intercept with Y-axis.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{images/hough_cartesian_equation}
	\caption{Line represented in Cartesian coordinates}
	\label{fig:line_cartesian}
\end{figure}
Other possible representation of a line is in polar coordinates (see Figure \ref{fig:line_polar}):
\begin{equation}
	\rho = x \cos\theta + y \sin\theta
\end{equation}
where $\rho$ represents the perpendicular distance from origin to the line and $\theta$ the angle the perpendicular makes with with the X-axis.
\begin{note}
	The OpenCV implementation of the Hough Lines algorithm returns the $\theta$ with values from range $[0, \pi]$ and allows negative $\rho$.
\end{note}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{images/hough_polar}
	\caption{Line represented in polar coordinates}
	\label{fig:line_polar}
\end{figure}

\paragraph{Hough space}
Let us now consider Image Space to be represented in polar coordinates. A line is then represented by some $\rho$ and $\theta$. We can draw such a point in $(\rho, \theta)$ coordinates that will be called \textbf{Hough space}.\cite{hough-lines}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{images/line_to_hough_space}
	\caption{Line represented in Hough space}
	\label{fig:line_hough_space}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{images/lines_to_hough_space}
	\caption{Bunch of lines with common point represented in Hough space}
	\label{fig:bunch_of_lines_hough_space}
\end{figure}
It turns out that a point in image space results in a sinusoid in Hough space.
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{images/point_to_sinusoid}
	\caption{Point in image space represented as sinusoid in Hough space}
	\label{fig:point_to_sinusoid}
\end{figure}
\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|}
		\hline
		\textbf{Image space} & \textbf{Hough space} \\ [0.5ex]
		\hline
		Straight line & Point \\ [0.5ex]
		\hline
		Point & Sinusoid \\ [0.5ex]
		\hline
	\end{tabular}
	\caption{Duality between image space and Hough space}
\end{table}
Finally, take a look at points that are forming a line in image space and their corresponding sinusoids in Hough space (Figure \ref{fig:line_detection_in_hough_space}). We can see, that all of the sinusoids intersect at exactly one point that represents the $(\rho, \theta)$ parameters of our line in image space.
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{images/points_to_sinusoids}
	\caption{Detecting a line in Hough space}
	\label{fig:line_detection_in_hough_space}
\end{figure}

\section{Finding the Projection}

\section{Reading the Code}

\section{Comparing Two Codes}